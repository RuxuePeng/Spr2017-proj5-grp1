##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Samsung using lag 4" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Samsung using lag 4" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Samsung using lag 4" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Samsung using lag 4" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Samsung using lag 4" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
xgb.plot.importance(importance_matrix = importance, main = "Google using lag 3" )
apple <- read.csv("~/Documents/Columbia/Spring2017/IEOR4725BigDatainFinance/FinalProject/Factors_AAPL_RavenPack.txt", sep = "", header = T)
lag_n_senti <- function(lag_n = 1, df) {
df <- df[, -c(1:4, 12)]
sign <- as.factor(ifelse(df$Stock.Return >0, 1, 0))
sent <- df[, c(1:7)]
sent_lag_n <- sent[-c((nrow(sent)-lag_n+1) : nrow(sent)), ]
fac_lag_n <- df[c(lag_n:(nrow(df)-1)), -c(1:7)]
sign_lag_n <- sign[c((lag_n+1):nrow(df))]
df_updated <- cbind(sent_lag_n, fac_lag_n, sign_lag_n)
colnames(df_updated)[ncol(df_updated)] <- "y"
df_updated <- apply(df_updated, 2, as.numeric)
return(as.data.frame(df_updated[, -c(11,13)]))
}
apple_lag_1 <- lag_n_senti(df = apple)
test.index <- c((nrow(apple_lag_1) - 20):nrow(apple_lag_1))
test=apple_lag_1[test.index, ]
train=apple_lag_1[-test.index, ]
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Google using lag 3" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
df = apple
df <- df[, -c(1:4, 12)]
sign <- as.factor(ifelse(df$Stock.Return >0, 1, 0))
sent <- df[, c(1:7)]
sent_lag_n <- sent[-c((nrow(sent)-lag_n+1) : nrow(sent)), ]
fac_lag_n <- df[c(lag_n:(nrow(df)-1)), -c(1:7)]
sign_lag_n <- sign[c((lag_n+1):nrow(df))]
df_updated <- cbind(sent_lag_n, fac_lag_n, sign_lag_n)
colnames(df_updated)[ncol(df_updated)] <- "y"
df_updated <- apply(df_updated, 2, as.numeric)
View(apple)
View(df_updated)
lag_n_senti <- function(lag_n = 1, df) {
df <- df[, -c(1:4, 12)]
sign <- as.factor(ifelse(df$Stock.Return >0, 1, 0))
sent <- df[, c(1:7)]
sent_lag_n <- sent[-c((nrow(sent)-lag_n+1) : nrow(sent)), ]
fac_lag_n <- df[c(lag_n:(nrow(df)-1)), -c(1:7)]
sign_lag_n <- sign[c((lag_n+1):nrow(df))]
df_updated <- cbind(sent_lag_n, fac_lag_n, sign_lag_n)
colnames(df_updated)[ncol(df_updated)] <- "y"
df_updated <- apply(df_updated, 2, as.numeric)
return(as.data.frame(df_updated[, -c(12,13)]))
}
apple_lag_1 <- lag_n_senti(df = apple)
test.index <- c((nrow(apple_lag_1) - 20):nrow(apple_lag_1))
test=apple_lag_1[test.index, ]
train=apple_lag_1[-test.index, ]
Train.x<- data.matrix(train[,-ncol(train)])
Train.y<-as.numeric(as.character(train[,ncol(train)]))
Test.x<-data.matrix(test[,-ncol(test)])
Test.y<-as.numeric(as.character(test[,ncol(test)]))
Train.x<- data.matrix(Train.x,rownames.force = NA)
Train.D <- xgb.DMatrix(data=Train.x,label=Train.y,missing = NaN)
Test.x<- data.matrix(Test.x,rownames.force = NA)
Test.D <- xgb.DMatrix(data=Test.x,label=Test.y,missing = NaN)
#write.csv(test_data_festures,"../data/test_final.csv",row.names = F)
######CV to select the best parameters:
depth.choice<- c(5,6,7,8)
eta.choice<- seq(0.1,0.5,0.1)
#Initilize:
error<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
iteration<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
##################
train.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
test.sd<-matrix(NA,nrow = length(eta.choice),ncol = length(depth.choice))
for (i in 1:length(depth.choice)) {
for (j in 1:length(eta.choice) ) {
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = eta.choice[j],
max_depth = depth.choice[i],
subsample = 0.5,
gamma = 0)
crossvalid <- xgb.cv( params = parameters,
data = Train.D,
nrounds = 100,
verbose = 1,
maximize = FALSE,
nfold = 5,
early_stopping_rounds = 8,
print_every_n = 1)
iteration[j,i]<-crossvalid$best_iteration
error[j,i]<-as.numeric(crossvalid$evaluation_log[crossvalid$best_iteration,4])
}
}
best.index<-which(error == min(error), arr.ind = TRUE)
depth.choose<-depth.choice[best.index[1,2]]
era.choose<-eta.choice[best.index[1,1]]
iteration.choose<-iteration[best.index[1,1],best.index[1,2]]
parameters <- list ( objective = "binary:logistic",
#booser = "gbtree",
eta = era.choose,
max_depth = depth.choose,
subsample = 0.5,
gamma = 0)
####################
##Train the model###
####################
fit_xgboost<-xgb.train( params              = parameters,
data                = Train.D,
nrounds             = 100,
verbose             = 1,
maximize            = FALSE)
importance <- xgb.importance(feature_names = colnames(Train.D), model = fit_xgboost)
xgb.plot.importance(importance_matrix = importance, main = "Google using lag 3" )
####################
##Get the prediction:
####################
###Testing:
prediction <- predict (fit_xgboost,Test.x)
prediction<-as.numeric(prediction > 0.5)
mean(prediction!=Test.y)
# 0.4285714 when lag = 2
xgb.plot.importance(importance_matrix = importance, main = "Apple using lag 1" )
1-0.38
1-0.28
1-0.3333333333
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
??highchartOutput
library("highcharter")
install.packages("highcharter")
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
require(devtools)
install_github("rCharts", "ramnathv")
library(rCharts)
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
install.packages("radarchart")
library(radarchart)
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
runApp('~/scr/ADS-Spring2017/Spr2017-proj5-grp1/app')
